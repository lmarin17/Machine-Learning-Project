---
title: "Machine Learning"
author: "Laura Marin"
date: "Wednesday, February 24, 2016"
output:
  html_document:
    keep_md: yes
---
#  Machine Learning Project
***


## Initial Setup and Objectives

Data provided by the Human Activity Recognition project contains movement readings combined with a 5-level factor variable of the exercise class.  

The objective of this model is to predict the correct class given a set of variable data. 

### Initial load of libaries and data

The data come from http://groupware.les.inf.puc-rio.br/har, which is provided by the Human Activity Recognition site
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H., Qualitative Activity Recognition of Weight Lifting Exercises.  Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13).  Stuttgart, Germany: ACM SIGCHI, 2013.  Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3yfgbGpDg

The site contains two files, a training and a testing file.  I've decided to further split the training file, so I'll load the testing file as the validation set which are the 20 submittal test cases.  

```{r}
library(caret)
library(e1071)

setwd("~/a_Research and Reading/Data Scientist Toolbox/Machine Learning")
rawtraining <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
validation <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

## Cross Validation

Upon loading the training set I immediately split it into a 75% training set and a 25% testing set using random subsampling. I chose random subsampling since this is a classification problem and this ensures the testing and training sets to have a similar distribution of each class.  This will allow me to explore various models on my training set, test the best candidates on my test set, then do a final test of my chosen model on my validatation set.  In this way I attempt to minimize the out of sample error as much as possible.

Looking at the training set I found there were 160 variables, many of which were mostly 0 or null.  To reduce the overhead of these variables, which I assume won't add intelligence to the model, I dropped these columns from training and testing.



```{r}
inTrain <- createDataPartition(rawtraining$classe, p = 3/4)[[1]]
trainingComplete <- rawtraining[ inTrain,]
testingComplete <- rawtraining[-inTrain,]

deleteCols <- c(1, 2,3,4,5,6,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,
                29,30,31,32,33,34,35,36,50,51,52,53,54,55,56,57,58,59,69,70,
                71,72,73,74,75,76,77,78,79,80,81,82,83,87,88,89,90,91,92,
                93,94,95,96,97,98,99,100,101,103,104,105,106,107,108,109,
                110,111,112,125,126,127,128,129,130,131,132,133,134,135,136,
                137,138,139,141,142,143,144,145,146,147,148,149,150)
 
training <- trainingComplete[, - deleteCols]
testing <- testingComplete[, -deleteCols]
```


There are 53 numeric variables left which very possibly would contain some correlation, at least between sets of variables.  I performed a PCA analysis to analyze these correlations, and determine how many PCA components I should include in my model to condense the number of variables involved but not lose significant predictive power.

```{r}
preProc <- prcomp(training[,-54])
summary(preProc)
```

The results show that about 99% of the variability is explained with 19 PCA components, and 99.9% with 29 components, far fewer than 53.  So by including the threshold = .999 in the preprocess options of the models explored below, I should be able to create models with a workable number of components, while still explaining much of the variability.


## Classification Modeling

This is a classification problem with a five-factor outcome.  Therefore, I looked at a few different classification and tree-based models, and compared their accuracies, then created an ensemble model to determine if a set of models would be a better option to move forward with.

###  A basic decision tree model  
Since this is a basic classification problem, I started with creating a simple decision tree model using pca preprocessing.  

```{r message=FALSE}
set.seed(62433)
modRpart <- train(training$classe ~ ., method = "rpart", preProcess="pca", data = training,
                  trControl = trainControl(preProcOptions = list(thresh = .999)))
predRpart <- predict(modRpart, training)
confusionMatrix(predRpart, training$classe)$overall[1]
```


### Gradient Boosting
The next option was a gradient boosting model, which is an ensemble of decision trees.  This should increase the accuracy of the rpart model.  Overfitting was a risk here, but validation using the testing dataset would give an unbiased view of the overfitting risk.

```{r message=FALSE}
set.seed(62433)
modGBM <- train(training$classe ~ ., method = "gbm", data = training, preProcess = "pca", verbose = FALSE,
                trControl = trainControl(preProcOptions = list(thresh = .999)))

predGBM <- predict(modGBM, training)
confusionMatrix(predGBM, training$classe)$overall[1]
```


### Linear Discriminant Analysis
This model tries to fit a number of lines through the data to determine the classifications.  

```{r message=FALSE}
set.seed(62433)
modLDA <- train(training$classe ~ ., method = "lda", data = training, preProcess = "pca", 
                trControl = trainControl(preProcOptions = list(thresh = .999)))

predLDA <- predict(modLDA, training)
confusionMatrix(predLDA, training$classe)$overall[1]
```



### Quadratic Discriminant Analysis
This model increases the complexity of the LDA model by dropping the assumption of linearity, and fits a number of quadratic functions through the data. 

```{r}
set.seed(62433)
modQDA <- train(training$classe ~ ., method = "qda", data = training, preProcess = "pca",
                trControl = trainControl(preProcOptions = list(thresh = .999)))

predQDA <- predict(modQDA, training)
confusionMatrix(predQDA, training$classe)$overall[1]
```


### Support Vector Machine
This model creates a number of hyperplanes through the data to classify the results. 

```{r message=FALSE}
set.seed(62433)
modSvm <- svm(training$classe ~ ., data = training)
predSvm <- predict(modSvm, training)
confusionMatrix(predSvm, training$classe)$overall[1]
```



### Performance of the Individual Models on the Testing Set
Based on the model results on the training set, I chose not to go forward with the basic decision tree model using the rpart algorithm, nor the linear discriminant model.  I tested each of the remaining individual models on the testing subset to confirm their effectiveness on a new dataset. This confirmation will reduce the out of sample error I would expect on the validation set.  

```{r message=FALSE}
predGBMTest <- predict(modGBM, testing)
confusionMatrix(predGBMTest, testing$classe)$overall[1]

predQDATest <- predict(modQDA, testing)
confusionMatrix(predQDATest, testing$classe)$overall[1]

predSvmTest <- predict(modSvm, testing)
confusionMatrix(predSvmTest, testing$classe)$overall[1]
```



### A random forest ensemble model 
Finally, I created an random forest ensemble model of the testing set predictions of the highest performing individual models, the GBM, QDA and SVM models.   


Testing Set Performance
```{r message=FALSE}
predDataTest <- data.frame(predGBMTest, predQDATest, predSvmTest, classe = testing$classe)
combModFitTest <- train(classe ~ ., method = "rf", data = predDataTest)
combPredTest <- predict(combModFitTest, predDataTest)
confusionMatrix(combPredTest, testing$classe)
```


The final performance is comparable to the highest performing individual model, the Support Vector Machines model, as seen visually by comparison barplots of the testing values, Support Vector Machines predictions and Random Forest Ensemble predictions.  

```{r}
par(mfrow = c(1,3))
barplot(table(testing$classe),col="blue", main = "Testing Values")
barplot(table(predSvmTest), col="red", main = "SVM Predictions")
barplot(table(combPredTest), col="red", main = "Ensemble Predictions")
```


## Conclusion
Doing a number of different models gave some assurance that the final Support Vector Machines model accuracy was sufficient for this exercise.  A final test on the validation set was successful with all 20 predictions correct.


